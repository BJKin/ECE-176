{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Modern Reimplementation of \"Context Encoders: Feature Learning by Inpainting\" using PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief description of project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "#dependencies for importing/reshaping data\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_central_mask_with_block(img, mask_size=64):\n",
    "    \"\"\"\n",
    "    For a 128x128 image, applies a central square mask (mask out a 64x64 area).\n",
    "    Returns:\n",
    "      masked_img: image with the central block replaced by black.\n",
    "      block_img: the 64x64 central region extracted from the original image.\n",
    "      coords: (left, top, right, bottom) of the masked region.\n",
    "    \"\"\"\n",
    "    width, height = img.size  # Should be 128x128\n",
    "    left = (width - mask_size) // 2\n",
    "    top = (height - mask_size) // 2\n",
    "    right = left + mask_size\n",
    "    bottom = top + mask_size\n",
    "\n",
    "    # Crop the block from the original image\n",
    "    block_img = img.crop((left, top, right, bottom))\n",
    "\n",
    "    # Create a masked version: paste black (0,0,0) in the central region\n",
    "    masked_img = img.copy()\n",
    "    masked_img.paste((0, 0, 0), (left, top, right, bottom))\n",
    "    \n",
    "    return masked_img, block_img, (left, top, right, bottom)\n",
    "\n",
    "def apply_random_region_mask_with_block(img, dropout_fraction=0.25):\n",
    "    \"\"\"\n",
    "    For a 227x227 image, apply a random rectangular mask.\n",
    "    The area of the mask is approximately dropout_fraction of the image.\n",
    "    Returns:\n",
    "      - masked_img: image with the random region masked (set to black).\n",
    "      - block_img: the cropped region that was masked out.\n",
    "      - coords: (left, top, right, bottom) of the masked region.\n",
    "    \"\"\"\n",
    "    width, height = img.size  # Expected to be 227x227\n",
    "    total_area = width * height\n",
    "    target_area = total_area * dropout_fraction\n",
    "\n",
    "    # Choose a random width between a minimum (say 20) and half of the width\n",
    "    w = random.randint(20, width // 2)\n",
    "    h = int(target_area / w)\n",
    "    h = min(h, height // 2)\n",
    "\n",
    "    max_left = width - w\n",
    "    max_top = height - h\n",
    "    left = random.randint(0, max_left)\n",
    "    top = random.randint(0, max_top)\n",
    "    right = left + w\n",
    "    bottom = top + h\n",
    "\n",
    "    block_img = img.crop((left, top, right, bottom))\n",
    "    masked_img = img.copy()\n",
    "    masked_img.paste((0, 0, 0), (left, top, right, bottom))\n",
    "    \n",
    "    return masked_img, block_img, (left, top, right, bottom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_masks_to_resized_images(root_dir, target_sizes={'128x128': (128, 128), '227x227': (227, 227)}):\n",
    "    \"\"\"\n",
    "    For each target size folder in root_dir, apply the corresponding masking strategy:\n",
    "      - For '128x128': apply a central mask (64x64) and extract the block.\n",
    "      - For '227x227': apply a random region mask (covering ~25% of the area).\n",
    "    Save the masked images and, for '128x128', also save the extracted block in separate directories.\n",
    "    \"\"\"\n",
    "    for size_label, dims in target_sizes.items():\n",
    "        # Input directory (resized images)\n",
    "        input_dir = os.path.join(root_dir, size_label)\n",
    "        # Output directory for masked images\n",
    "        output_masked_base = os.path.join(root_dir, f\"{size_label}_masked\")\n",
    "        os.makedirs(output_masked_base, exist_ok=True)\n",
    "        print(f\"Processing {size_label} images from {input_dir} -> saving masked images to {output_masked_base}\")\n",
    "        \n",
    "        # For 128x128, also prepare a directory for the extracted block\n",
    "        if size_label == '128x128':\n",
    "            output_block_base = os.path.join(root_dir, f\"{size_label}_block\")\n",
    "            os.makedirs(output_block_base, exist_ok=True)\n",
    "            #print(f\"Saving extracted blocks to {output_block_base}\")\n",
    "        \n",
    "        # Walk through the input directory recursively\n",
    "        for current_dir, subdirs, files in os.walk(input_dir):\n",
    "            rel_dir = os.path.relpath(current_dir, input_dir)\n",
    "            output_masked_subdir = os.path.join(output_masked_base, rel_dir)\n",
    "            os.makedirs(output_masked_subdir, exist_ok=True)\n",
    "            \n",
    "            if size_label == '128x128':\n",
    "                output_block_subdir = os.path.join(root_dir, f\"{size_label}_block\", rel_dir)\n",
    "                os.makedirs(output_block_subdir, exist_ok=True)\n",
    "            \n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    file_path = os.path.join(current_dir, file)\n",
    "                    try:\n",
    "                        with Image.open(file_path) as img:\n",
    "                            img = img.convert('RGB')\n",
    "                            if img.size != dims:\n",
    "                                print(f\"Skipping {file_path}: size {img.size} does not match expected {dims}\")\n",
    "                                continue\n",
    "                            \n",
    "                            if size_label == '128x128':\n",
    "                                # Apply central mask for 128x128 images\n",
    "                                masked_img, block_img, coords = apply_central_mask_with_block(img, mask_size=64)\n",
    "                                masked_save_path = os.path.join(output_masked_subdir, file)\n",
    "                                block_save_path = os.path.join(output_block_subdir, file)\n",
    "                                masked_img.save(masked_save_path)\n",
    "                                block_img.save(block_save_path)\n",
    "                                #print(f\"Processed {file}: saved masked image to {masked_save_path} and block to {block_save_path}\")\n",
    "                            elif size_label == '227x227':\n",
    "                                # Apply random region mask for 227x227 images\n",
    "                                masked_img, _, coords = apply_random_region_mask_with_block(img, dropout_fraction=0.25)\n",
    "                                masked_save_path = os.path.join(output_masked_subdir, file)\n",
    "                                masked_img.save(masked_save_path)\n",
    "                                #print(f\"Processed {file}: saved masked image to {masked_save_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Example usage for masking:\n",
    "base_dir = './data'\n",
    "apply_masks_to_resized_images(base_dir, target_sizes={'128x128': (128, 128), '227x227': (227, 227)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_module import InpaintingDataset\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "lr_gen = 1e-4\n",
    "lr_disc = 1e-5\n",
    "lambda_rec = 0.999  # reconstruction loss weight\n",
    "lambda_adv = 0.001  # adversarial loss weight\n",
    "\n",
    "# Define image transformation: resize images to 128x128 and convert to tensor\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader.\n",
    "# This dataset loads images from \"./data/128x128\" (unmasked) and applies a central mask on the fly.\n",
    "dataset_dir = './data/128x128'\n",
    "train_dataset = InpaintingDataset(root_dir=dataset_dir, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Import models from your models directory\n",
    "from models.FeatureLearner import ContextEncoder\n",
    "from models.SemanticInpainter import Discriminator\n",
    "\n",
    "# Instantiate models and move them to device\n",
    "generator = ContextEncoder().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Define loss functions\n",
    "criterion_rec = nn.MSELoss()   # Reconstruction (L2) loss\n",
    "criterion_adv = nn.BCELoss()   # Adversarial loss (BCE)\n",
    "\n",
    "# Define optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.999))\n",
    "\n",
    "# Helper function to visualize inpainting results\n",
    "def visualize_inpainting(masked, generated, ground_truth, idx=0):\n",
    "    masked_img = transforms.ToPILImage()(masked[idx].cpu())\n",
    "    gen_img = transforms.ToPILImage()(generated[idx].cpu().detach())\n",
    "    gt_img = transforms.ToPILImage()(ground_truth[idx].cpu())\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axs[0].imshow(masked_img)\n",
    "    axs[0].set_title(\"Masked Input\")\n",
    "    axs[1].imshow(gen_img)\n",
    "    axs[1].set_title(\"Generated Output\")\n",
    "    axs[2].imshow(gt_img)\n",
    "    axs[2].set_title(\"Ground Truth Block\")\n",
    "    plt.show()\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (masked_imgs, blocks) in enumerate(train_loader):\n",
    "        masked_imgs = masked_imgs.to(device)\n",
    "        blocks = blocks.to(device)\n",
    "        \n",
    "        # --- Update Discriminator ---\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        # Compute discriminator outputs for real blocks and create matching labels\n",
    "        outputs_real = discriminator(blocks)\n",
    "        real_labels = torch.ones_like(outputs_real, device=device)\n",
    "        d_loss_real = criterion_adv(outputs_real, real_labels)\n",
    "        \n",
    "        # Generate fake blocks and compute discriminator outputs and matching labels\n",
    "        fake_blocks = generator(masked_imgs)\n",
    "        outputs_fake = discriminator(fake_blocks.detach())\n",
    "        fake_labels = torch.zeros_like(outputs_fake, device=device)\n",
    "        d_loss_fake = criterion_adv(outputs_fake, fake_labels)\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # --- Update Generator ---\n",
    "        generator.zero_grad()\n",
    "        fake_blocks = generator(masked_imgs)\n",
    "        outputs_gen = discriminator(fake_blocks)\n",
    "        real_labels_gen = torch.ones_like(outputs_gen, device=device)\n",
    "        adv_loss = criterion_adv(outputs_gen, real_labels_gen)\n",
    "        \n",
    "        # Resize fake_blocks to match ground truth dimensions (128x128) for reconstruction loss\n",
    "        fake_blocks_resized = torch.nn.functional.interpolate(fake_blocks, size=blocks.shape[2:], mode='bilinear', align_corners=False)\n",
    "        rec_loss = criterion_rec(fake_blocks_resized, blocks)\n",
    "        \n",
    "        g_loss = lambda_rec * rec_loss + lambda_adv * adv_loss\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(train_loader)}] \"\n",
    "                  f\"d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, \"\n",
    "                  f\"rec_loss: {rec_loss.item():.4f}, adv_loss: {adv_loss.item():.4f}\")\n",
    "            visualize_inpainting(masked_imgs, fake_blocks, blocks)\n",
    "    \n",
    "    # Save model checkpoints after each epoch\n",
    "    torch.save(generator.state_dict(), f\"generator_epoch_{epoch+1}.pth\")\n",
    "    torch.save(discriminator.state_dict(), f\"discriminator_epoch_{epoch+1}.pth\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
